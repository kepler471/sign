{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# ASL sign challenge exploration\n",
    "\n",
    "Data includes 3000 images per sign in ASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from skimage.transform import resize\n",
    "%matplotlib inline\n",
    "\n",
    "# kaggle input data path (change when running a local notebook)\n",
    "TRAIN_DIR = \"../input/asl_alphabet_train/asl_alphabet_train/\"\n",
    "\n",
    "# Sort signs A through Z then del, nothing, space\n",
    "class_folders = np.sort(os.listdir(TRAIN_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in data to numpy array, fucking slow on cpu\n",
    "### Change this from a loop? Could use DataFrame\n",
    "\n",
    "labels = []\n",
    "data = []\n",
    "cutoff = 1000\n",
    "\n",
    "label_no = 0\n",
    "for class_name in class_folders:\n",
    "    clear_output()\n",
    "    \n",
    "    label_dir = TRAIN_DIR + class_name + \"/\"\n",
    "    class_files = os.listdir(label_dir)\n",
    "    for i,file in enumerate(class_files):\n",
    "        \n",
    "        # print(\"On class {} and file {}\".format(label_no, i))\n",
    "        img = np.array(Image.open(label_dir + file))\n",
    "        \n",
    "        # removing the blue border on all images\n",
    "        trim_img = img[15:-10,15:-10]\n",
    "        \n",
    "        # Downsizing images to 64x64\n",
    "        data.append(resize(trim_img, (64,64)))\n",
    "        labels.append(label_no)\n",
    "        \n",
    "        # Cutoff samples for each sign to speed testing\n",
    "        if i > cutoff:\n",
    "            break\n",
    "    label_no += 1\n",
    "    \n",
    "    \n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6521886585993fd0850e44d356a9bb017df8473e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad8aac26ac319cb2969fcca7c01b708913bed957"
   },
   "outputs": [],
   "source": [
    "np.shape(labels)\n",
    "np.shape(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "00a1cde56dc977cbd26cd2444a24ed789a8232db"
   },
   "outputs": [],
   "source": [
    "# checking what the formatted data looks like\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "\n",
    "fig=plt.figure(figsize=(18, 18), dpi= 80, facecolor='w', edgecolor='k')\n",
    "rows = 5\n",
    "columns = 6\n",
    "for i in range(1,30):\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(resize(data[i*cutoff,:,:,:], (64,64)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e936866649e2594bf9e883cae249b4c63137cf9b"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc8d0cb119905006c05edfc74b04a4b2bd50df3d"
   },
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_dir = \"../input/asl_alphabet_train/asl_alphabet_train\"\n",
    "target_size = (64, 64)\n",
    "target_dims = (64, 64, 3) # add channel for RGB\n",
    "\n",
    "val_frac = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "data_augmentor = ImageDataGenerator(samplewise_center=True, \n",
    "                                    samplewise_std_normalization=True, \n",
    "                                    validation_split=val_frac)\n",
    "\n",
    "train_generator = data_augmentor.flow_from_directory(data_dir, \n",
    "                target_size=target_size, batch_size=batch_size, \n",
    "                shuffle=True, subset=\"training\")\n",
    "val_generator = data_augmentor.flow_from_directory(data_dir, \n",
    "                target_size=target_size, batch_size=batch_size, \n",
    "                subset=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c172acd9a7f6a116fff577af0fae8a6eb8fef91d"
   },
   "source": [
    "Might be low res but will run better for a first passthrough. Blue borders are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f971a76b80954e42a0f1b8f9351b838fb687ff47"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Take a portion of data for validation\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(data, \n",
    "                          labels, test_size=0.1, random_state=4)\n",
    "\n",
    "Y_train = np_utils.to_categorical(Y_train)\n",
    "Y_val = np_utils.to_categorical(Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bebd03fda2ec542be3a4cd292748061f8c5c384"
   },
   "source": [
    "## Building the model using Keras\n",
    "\n",
    "### Network\n",
    "\n",
    "The network consists of two main components :\n",
    "\n",
    "#### Convolutional layers : \n",
    "\n",
    "the convolutional layer is responsible for the convolutional operation in which feature maps identifies features in the images. and is usually followed by two types of layers which are :\n",
    "\n",
    "   Dropout : Dropout is a regulization technique where you turn off part of the network's layers randomally to increase regulization and hense decrease overfitting. We use when the training set accuracy is muuch higher than the test set accuracy.\n",
    "    Max Pooling : The maximum output in a rectangular neighbourhood. It is used to make the network more flexible to slight changes and decrease the network computationl expenses by extracting the group of pixels that are highly contributing to each feature in the feature maps in the layer.\n",
    "\n",
    "#### Dense layers : \n",
    "\n",
    "The dense layer is a fully connected layer that comes after the convolutional layers and they give us the output vector of the Network.\n",
    "    \n",
    "[Number of parameteres for convolution layers?](https://datascience.stackexchange.com/questions/17064/number-of-parameters-for-convolution-layers)\n",
    "\n",
    "### Compilation\n",
    "Before training a model, you need to configure the learning process, which is done via the  `compile` method, receiving 3 arguments: `optimizer`, `loss function`,  `list of metrics`. \n",
    "\n",
    "[Keras Docs: Compilation](https://keras.io/getting-started/sequential-model-guide/#compilation)\n",
    "\n",
    "[Overview of gradient descent algorithms](http://ruder.io/optimizing-gradient-descent/)\n",
    "\n",
    "Adam is a method for Stochastic Optimization. See the following papers: \n",
    "\n",
    "[Adam - A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980v8)\n",
    "\n",
    "[On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ)\n",
    "\n",
    "Cost function : \n",
    "It is a measure of the overall loss in our network after assigning values to the parameters during the forward phase so it indicates how well the parameters were chosen during the forward probagation phase.\n",
    "\n",
    "Optimizer : \n",
    "It is the gradiant descent algorithm that is used. We use it to minimize the cost function to approach the minimum point. We are using adam optimizer which is one of the best gradient descent algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33eb1f3d75a2126b13a36e085150a24852636c04"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "run_name = '10 epochs lr 0.005.ckpt'\n",
    "# Build a model using 2D Convolution\n",
    "def training(weights_path):\n",
    "    model = Sequential()\n",
    "    \n",
    "    #layers\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (3,3), padding = 'Same', \n",
    "                    activation = 'relu', input_shape = (64,64,3)))\n",
    "    model.add(Conv2D(filters = 16, kernel_size = (3,3), padding = 'Same', \n",
    "                    activation = 'relu'))\n",
    "\n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same',\n",
    "                    activation = 'relu'))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (3,3), padding = 'Same',\n",
    "                    activation = 'relu'))\n",
    "    \n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',\n",
    "                    activation = 'relu'))\n",
    "    model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same',\n",
    "                    activation = 'relu'))\n",
    "    \n",
    "    model.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(label_no, activation = 'softmax'))\n",
    "    \n",
    "    optimizer = Adam(lr=0.005)\n",
    "    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', \n",
    "                    metrics = ['accuracy'])\n",
    "    \n",
    "    if os.path.isfile(weights_path):\n",
    "        model.load_weights(weights_path)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = training(run_name)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "461429a4f53b9ac1b4ed4735ff3e511d3459a268"
   },
   "outputs": [],
   "source": [
    "# testing package setup (for local machine)\n",
    "from tensorflow.python.client import device_lib\n",
    "import sys\n",
    "\n",
    "print(sys.version)\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98bbc0befa8873bcc63ff62d8d3a00a20aaa645a"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ad0b97d70a39ad8bed9d6a2ec8f6d9c6c76deedb"
   },
   "outputs": [],
   "source": [
    "saver = ModelCheckpoint(run_name, monitor=\"val_loss\")\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size = batch_size, \n",
    "                    epochs = epochs, validation_data = (X_val, Y_val),\n",
    "                    verbose = 2, callbacks = [saver])\n",
    "\n",
    "#history = model.fit_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7bcc7ee8261c4d19366b7ff45d79191cf66ef154"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f7973af74b4ae9a623961267deff75e21810057"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "fig=plt.figure(figsize=(15, 15))\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes to one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(29))\n",
    "plt.savefig(run_name[:-5]+'_conf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36ac0fab0d3d1174ec0b86806f9c07484a469354"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61f526ffddba37d990630cb523bad2439312ca04"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "y_val = [np.argmax(one_hot) for one_hot in Y_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ccfb0871c0557b8d4a04308720bc668dcf5bc5ba"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e2f777de3f309ea4c35d4fab9fb08e58adfe99d"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for keys in ['val_loss', 'loss']:\n",
    "    plt.plot(np.arange(0,10), history.history[keys], label=keys)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "for keys in ['val_acc', 'acc']:\n",
    "    plt.plot(np.arange(0,10), history.history[keys], label=keys)\n",
    "plt.legend()\n",
    "plt.title(max(history.history['val_acc']))\n",
    "plt.savefig(run_name[:-5]+'_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab818b9608ded0191ef9e7be6a4f7873c90900f2"
   },
   "outputs": [],
   "source": [
    "history.history['val_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b18341e3b0f3cb669cb217a47919f5db26ba0dde"
   },
   "source": [
    "# View false positives\n",
    "\n",
    "Lets have a look at where the network hasn't performed well, it may point towards flaws in the method/model architecture or flaws with the data such as poor lighting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb9bc7d2ceb7038cd0c37e60e03c58f79664f375"
   },
   "outputs": [],
   "source": [
    "# Predict all images n the training set\n",
    "train_pred = model.predict(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f30446f0afd84e01c2dbdf991544b4995520367a"
   },
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "102468a6bf5a9fc00088bbe3cdfdba49bab88ba9"
   },
   "source": [
    "# Analyse Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "80eb55aecc336cd3c7ce0b87560b8f0c36ec1350"
   },
   "source": [
    "Lets have a look at the model training curves to see if we have set the hyperparameters corectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e76f74e377e4a4df765fac42e52424dfc1a3da61"
   },
   "outputs": [],
   "source": [
    "from keras import metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
